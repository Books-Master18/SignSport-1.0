"""
# train_model.py
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset

# === 1. –î–ê–ù–ù–´–ï ===
dataset = load_dataset("csv", data_files="train.csv", encoding="utf-8")

# === 2. –ú–ï–¢–ö–ò ===
SPORTS = [
    "–§—É—Ç–±–æ–ª", "–ì–∞–Ω–¥–±–æ–ª", "–í–æ–¥–Ω–æ–µ –ø–æ–ª–æ", "–í–æ–ª–µ–π–±–æ–ª", "–ü–ª–∞–≤–∞–Ω–∏–µ",
    "–§–∏–≥—É—Ä–Ω–æ–µ –∫–∞—Ç–∞–Ω–∏–µ", "–¢—è–∂–µ–ª–∞—è –∞—Ç–ª–µ—Ç–∏–∫–∞", "–¢–µ–Ω–Ω–∏—Å",
    "–•–æ–∫–∫–µ–π", "–§–µ—Ö—Ç–æ–≤–∞–Ω–∏–µ", "–ê–∫—Ä–æ–±–∞—Ç–∏–∫–∞", "–®–∞—Ö–º–∞—Ç—ã", "–ö–æ–Ω–Ω—ã–π —Å–ø–æ—Ä—Ç"
]
label2id = {sport: i for i, sport in enumerate(SPORTS)}
id2label = {i: sport for i, sport in enumerate(SPORTS)}

# === 3. –ú–û–î–ï–õ–¨ ===
model_name = "cointegrated/rubert-tiny2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(SPORTS),
    label2id=label2id,
    id2label=id2label
)

# === 4. –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ===
def tokenize(batch):
    encodings = tokenizer(
        batch["text"],
        padding=True,
        truncation=True,
        max_length=128
    )
    labels = [label2id[label] for label in batch["label"]]
    encodings["labels"] = labels
    return encodings

tokenized = dataset.map(
    tokenize,
    batched=True,
    batch_size=32,
    remove_columns=["text", "label"]
)

# === 5. –û–ë–£–ß–ï–ù–ò–ï ===
training_args = TrainingArguments(
    output_dir="signsport-model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_strategy="no",
    logging_steps=10,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    tokenizer=tokenizer
)

print("üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å...")
trainer.train()

# === 6. –°–û–•–†–ê–ù–ï–ù–ò–ï ===
model.save_pretrained("signsport-model")
tokenizer.save_pretrained("signsport-model")
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É 'signsport-model'")
"""